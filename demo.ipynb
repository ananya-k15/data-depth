{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14229331-ac21-4540-b461-9f9d0bf658cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DELL/opt/anaconda3/envs/data-depth/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/DELL/opt/anaconda3/envs/data-depth/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CC4BC91F-8B6A-3F9A-B9EB-A2B9D578E202> /Users/DELL/opt/anaconda3/envs/data-depth/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <5310731F-64FC-310E-B88B-E06F64BB3F98> /Users/DELL/opt/anaconda3/envs/data-depth/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import eigh\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4678db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth.multivariate import *\n",
    "from numpy.random import RandomState\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c05a8-11e6-4a8a-b663-46531fd5c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model Definition\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, z_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(x_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, z_dim*2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, x_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de4867-5856-4f9f-942f-bdac2b0cfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Train VAE\n",
    "def train_model(model, dataloader, learning_rate, num_epochs, max_batches=None):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for bno, batch in enumerate(dataloader):\n",
    "            if max_batches != None and bno > max_batches :\n",
    "                break\n",
    "            x, _ = batch\n",
    "            x = x.view(x.size(0), -1)\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, z, mu, logvar = model(x)\n",
    "            loss = loss_function(recon_x, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f847c-7791-4249-b61c-b35afc776fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "def load_data(normal_class=0):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "    fashion_mnist_train = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=False)\n",
    "    fashion_mnist_test = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=False)\n",
    "    \n",
    "    # Filter out only the normal class\n",
    "    train_indices = [i for i, (_, label) in enumerate(fashion_mnist_train) if label == normal_class]\n",
    "    normal_train = Subset(fashion_mnist_train, train_indices)\n",
    "    \n",
    "    return normal_train, fashion_mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462474a5-fd39-4be3-8749-ba5e889bd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mahalanobis depth\n",
    "def compute_mahalanobis_depth(latent_space):\n",
    "    mean = np.mean(latent_space, axis=0)\n",
    "    cov = np.cov(latent_space, rowvar=False)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    depth = np.array([distance.mahalanobis(x, mean, inv_cov) for x in latent_space])\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132fc76-f7ba-4988-a857-d37eed1d13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main function\n",
    "# def main():\n",
    "normal_train, fashion_mnist_test = load_data(normal_class=0)  # Class 0: T-shirt/top\n",
    "\n",
    "dataloader = DataLoader(normal_train, batch_size=128, shuffle=False)\n",
    "\n",
    "# Define VAE model\n",
    "x_dim = 28 * 28\n",
    "hidden_dim = 256\n",
    "latent_dim = 50\n",
    "model = VAE(x_dim=x_dim, hidden_dim=hidden_dim, z_dim=latent_dim)\n",
    "\n",
    "# Train VAE\n",
    "MAX_STEPS = 300\n",
    "BATCH_SIZE = 150\n",
    "trained_model = train_model(model, dataloader, learning_rate=1e-4, num_epochs=10, max_batches=MAX_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7482d-51cc-445a-ace6-c3d12e555c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latent representations\n",
    "latent_space = []\n",
    "labels = []\n",
    "depths = np.array([])\n",
    "trained_model.eval()\n",
    "sample_no = 1\n",
    "ind = 0\n",
    "with torch.no_grad():\n",
    "    for batch in fashion_mnist_test:\n",
    "        if sample_no % BATCH_SIZE == 0: \n",
    "            bno = sample_no % BATCH_SIZE\n",
    "            print(\"Batch number\", bno, \" slicing from\", ind, \"to\", ind+150)\n",
    "            batch_latent_space = np.array(latent_space[ind:ind+150])\n",
    "            batch_labels = np.array(labels[ind:ind+150])\n",
    "            batch_depths = halfspace(batch_labels, batch_latent_space)\n",
    "            print(\"the function gave me\", len(batch_latent_space))\n",
    "            depths = np.concatenate((depths, batch_depths), axis = 0)\n",
    "            ind += 150\n",
    "        if sample_no >= MAX_STEPS : \n",
    "            break\n",
    "        # get latent shape\n",
    "        x, y = batch\n",
    "        x = x.view(-1, x.size(0))\n",
    "        _, z, _, _ = trained_model(x)\n",
    "        latent_space.append(z.cpu().numpy())\n",
    "        # y-labels\n",
    "        labels.append(torch.tensor(y).cpu().numpy())\n",
    "        sample_no = len(latent_space)\n",
    "    \n",
    "latent_space = np.concatenate(latent_space, axis=0)\n",
    "labels = np.concatenate([labels], axis=0)\n",
    "# depths = np.concatenate([batch_depths], axis=0)\n",
    "\n",
    "print(latent_space.shape)\n",
    "print(labels.shape)\n",
    "print(len(depths))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22012f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for anomalies (e.g., top 5% deepest points)\n",
    "print(len(depths))\n",
    "threshold = np.percentile(depths, 5)\n",
    "print(threshold)\n",
    "anomalies = depths <= threshold\n",
    "\n",
    "# Visualize latent space with anomalies\n",
    "plt.scatter(latent_space[:, 0], latent_space[:, 1], c='blue', label='Normal')\n",
    "plt.scatter(latent_space[anomalies, 0], latent_space[anomalies, 1], c='red', label='Anomalies')\n",
    "plt.title('Latent Space')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcec912",
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_depths = depths[anomalies]\n",
    "anom_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3743c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(depths), np.max(depths))\n",
    "print(np.std(depths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original and reconstructed anomalies\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "for i in anomaly_indices:  # Show up to 10 anomalies\n",
    "    original = fashion_mnist_test[i][0].view(28, 28).numpy()\n",
    "    with torch.no_grad():\n",
    "        org = fashion_mnist_test[i][0]\n",
    "        reconstructed, _, _, _ = trained_model(org.view(-1, org.size(0)))\n",
    "    reconstructed = reconstructed.view(28, 28).numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(original, cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[1].imshow(reconstructed, cmap='gray')\n",
    "    axes[1].set_title('Reconstructed')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c51b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
