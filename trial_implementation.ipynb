{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b36281-c303-4ef9-a542-0700cf81d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 01:48:32.122724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0699de53-1f0d-4c26-9d03-32a1f8b6f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.reshape(x_train, (len(x_train), -1))\n",
    "x_test = np.reshape(x_test, (len(x_test), -1))\n",
    "\n",
    "# Combine train and test data\n",
    "data = np.concatenate((x_train, x_test), axis=0)\n",
    "\n",
    "# Define the dimensions\n",
    "original_dim = data.shape[1]\n",
    "latent_dim = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae00cb0-fac8-4804-9451-0743c315763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 01:48:34.331004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "inputs = Input(shape=(original_dim,))\n",
    "h = layers.Dense(64, activation='relu')(inputs)\n",
    "h = layers.Dense(32, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Latent space\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90260486-db77-42a3-a6eb-f966fd45b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_h = layers.Dense(32, activation='relu')\n",
    "decoder_h2 = layers.Dense(64, activation='relu')\n",
    "decoder_mean = layers.Dense(original_dim, activation='sigmoid')\n",
    "\n",
    "h_decoded = decoder_h(z)\n",
    "h_decoded2 = decoder_h2(h_decoded)\n",
    "x_decoded_mean = decoder_mean(h_decoded2)\n",
    "\n",
    "# Define the VAE model\n",
    "vae = Model(inputs, x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aab07b7-b0ec-4256-a18b-45c46e3ff92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 47.5404 - val_loss: 41.1354\n",
      "Epoch 2/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 40.0705 - val_loss: 39.0328\n",
      "Epoch 3/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 38.6539 - val_loss: 37.9707\n",
      "Epoch 4/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 37.7262 - val_loss: 37.1345\n",
      "Epoch 5/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 36.9796 - val_loss: 36.5001\n",
      "Epoch 6/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 36.4319 - val_loss: 35.9548\n",
      "Epoch 7/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 36.0388 - val_loss: 35.7329\n",
      "Epoch 8/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 35.6913 - val_loss: 35.3658\n",
      "Epoch 9/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 35.4190 - val_loss: 35.2695\n",
      "Epoch 10/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 35.1731 - val_loss: 35.1051\n",
      "Epoch 11/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.9861 - val_loss: 34.7873\n",
      "Epoch 12/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.8340 - val_loss: 34.6816\n",
      "Epoch 13/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.6707 - val_loss: 34.5434\n",
      "Epoch 14/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.5526 - val_loss: 34.5090\n",
      "Epoch 15/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.4653 - val_loss: 34.4290\n",
      "Epoch 16/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.3697 - val_loss: 34.3387\n",
      "Epoch 17/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.3024 - val_loss: 34.2964\n",
      "Epoch 18/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.2412 - val_loss: 34.2811\n",
      "Epoch 19/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.1701 - val_loss: 34.0797\n",
      "Epoch 20/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.1419 - val_loss: 34.1426\n",
      "Epoch 21/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 34.0697 - val_loss: 34.0099\n",
      "Epoch 22/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.9973 - val_loss: 34.1117\n",
      "Epoch 23/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.9341 - val_loss: 34.0199\n",
      "Epoch 24/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.9022 - val_loss: 34.0141\n",
      "Epoch 25/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.8780 - val_loss: 33.8707\n",
      "Epoch 26/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.8434 - val_loss: 33.8434\n",
      "Epoch 27/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.7856 - val_loss: 33.9953\n",
      "Epoch 28/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.7666 - val_loss: 33.9046\n",
      "Epoch 29/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.7391 - val_loss: 33.7735\n",
      "Epoch 30/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.6912 - val_loss: 33.8371\n",
      "Epoch 31/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.6948 - val_loss: 33.7463\n",
      "Epoch 32/50\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 33.6337 - val_loss: 33.7801\n",
      "Epoch 33/50\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 33.6250 - val_loss: 33.6582\n",
      "Epoch 34/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.5888 - val_loss: 33.8757\n",
      "Epoch 35/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.5709 - val_loss: 33.6450\n",
      "Epoch 36/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.5131 - val_loss: 33.7897\n",
      "Epoch 37/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.5142 - val_loss: 33.6681\n",
      "Epoch 38/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.4774 - val_loss: 33.6804\n",
      "Epoch 39/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.4737 - val_loss: 33.6661\n",
      "Epoch 40/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.4612 - val_loss: 33.5658\n",
      "Epoch 41/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.3995 - val_loss: 33.6422\n",
      "Epoch 42/50\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 33.3958 - val_loss: 33.6229\n",
      "Epoch 43/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.3779 - val_loss: 33.6653\n",
      "Epoch 44/50\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 33.3731 - val_loss: 33.5122\n",
      "Epoch 45/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.3312 - val_loss: 33.4733\n",
      "Epoch 46/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.3345 - val_loss: 33.5073\n",
      "Epoch 47/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.3152 - val_loss: 33.4276\n",
      "Epoch 48/50\n",
      "1750/1750 [==============================] - 3s 2ms/step - loss: 33.3041 - val_loss: 33.5163\n",
      "Epoch 49/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.2597 - val_loss: 33.6555\n",
      "Epoch 50/50\n",
      "1750/1750 [==============================] - 3s 1ms/step - loss: 33.2872 - val_loss: 33.5155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd97890a460>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the loss\n",
    "reconstruction_loss = tf.keras.losses.mean_squared_error(inputs, x_decoded_mean)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(data, data, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee2f24c-47a7-4c46-918f-61ae88fa01ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188/2188 [==============================] - 1s 561us/step\n",
      "          x         y         z\n",
      "0  0.340344  0.898225 -0.130393\n",
      "1  1.119578  1.195147 -0.598310\n",
      "2  1.328761 -1.318005  0.280387\n",
      "3 -1.772535  0.642607 -0.323692\n",
      "4 -0.227911 -0.933447  0.059159\n"
     ]
    }
   ],
   "source": [
    "# Encoder model to transform data to latent space\n",
    "encoder = Model(inputs, z_mean)\n",
    "\n",
    "# Transform the dataset\n",
    "latent_representations = encoder.predict(data)\n",
    "\n",
    "# Convert the latent representations to a pandas DataFrame\n",
    "latent_df = pd.DataFrame(latent_representations, columns=['x', 'y', 'z'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(latent_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f758e2a-c3f7-412a-8258-716edb2e5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth.multivariate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40720d6-586b-4d6b-99d5-cd2b897d1d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,) (70000, 3)\n"
     ]
    }
   ],
   "source": [
    "combined_data = latent_df.to_numpy()\n",
    "combined_x = latent_df['x'].to_numpy()\n",
    "print(combined_x.shape, combined_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29e20a8-fda5-41cf-8417-f8ea2ac174ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "halfspace_depths = halfspace(combined_x[:1000], combined_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e814e212-71af-4568-82d6-2050ae769acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.054 0.044 0.255 0.324 0.059 0.088 0.031 0.    0.129 0.008 0.063 0.082\n",
      " 0.23  0.065 0.127 0.302 0.283 0.01  0.01  0.006 0.003 0.062 0.    0.02\n",
      " 0.098 0.    0.016 0.076 0.046 0.049 0.028 0.089 0.172 0.063 0.063 0.05\n",
      " 0.    0.003 0.006 0.027 0.    0.012 0.11  0.243 0.136 0.245 0.035 0.043\n",
      " 0.271 0.089 0.077 0.141 0.001 0.1   0.058 0.158 0.126 0.212 0.094 0.082\n",
      " 0.096 0.064 0.088 0.322 0.023 0.144 0.132 0.071 0.    0.    0.039 0.224\n",
      " 0.078 0.041 0.004 0.279 0.    0.085 0.082 0.257 0.04  0.087 0.005 0.\n",
      " 0.098 0.363 0.    0.057 0.099 0.    0.032 0.328 0.044 0.153 0.002 0.198\n",
      " 0.011 0.047 0.    0.101 0.083 0.012 0.002 0.004 0.138 0.045 0.07  0.092\n",
      " 0.071 0.327 0.162 0.228 0.089 0.243 0.23  0.088 0.035 0.002 0.05  0.024\n",
      " 0.207 0.154 0.066 0.104 0.353 0.094 0.035 0.157 0.068 0.031 0.163 0.12\n",
      " 0.005 0.054 0.189 0.171 0.06  0.071 0.104 0.223 0.141 0.152 0.132 0.01\n",
      " 0.216 0.009 0.    0.012 0.268 0.009 0.005 0.    0.025 0.291 0.    0.\n",
      " 0.136 0.013 0.003 0.15  0.282 0.    0.006 0.    0.007 0.    0.243 0.046\n",
      " 0.008 0.001 0.002 0.399 0.066 0.019 0.006 0.    0.03  0.    0.004 0.01\n",
      " 0.156 0.209 0.253 0.429 0.009 0.223 0.027 0.234 0.272 0.003 0.087 0.038\n",
      " 0.118 0.41  0.024 0.    0.104 0.003 0.007 0.012 0.042 0.011 0.029 0.001\n",
      " 0.062 0.    0.034 0.069 0.071 0.321 0.06  0.071 0.003 0.026 0.004 0.079\n",
      " 0.005 0.004 0.076 0.055 0.045 0.128 0.055 0.    0.171 0.074 0.145 0.001\n",
      " 0.142 0.048 0.04  0.323 0.008 0.065 0.011 0.128 0.068 0.023 0.226 0.104\n",
      " 0.094 0.02  0.045 0.124 0.079 0.124 0.192 0.078 0.28  0.018 0.166 0.03\n",
      " 0.215 0.128 0.076 0.071 0.003 0.065 0.223 0.004 0.125 0.009 0.114 0.07\n",
      " 0.072 0.023 0.168 0.    0.082 0.004 0.187 0.089 0.052 0.    0.203 0.031\n",
      " 0.    0.012 0.015 0.    0.141 0.195 0.071 0.139 0.049 0.299 0.089 0.142\n",
      " 0.14  0.047 0.    0.046 0.333 0.03  0.009 0.113 0.138 0.037 0.128 0.104\n",
      " 0.075 0.005 0.16  0.    0.162 0.273 0.    0.337 0.023 0.014 0.288 0.005\n",
      " 0.142 0.    0.032 0.154 0.007 0.002 0.059 0.039 0.    0.163 0.095 0.003\n",
      " 0.239 0.154 0.268 0.013 0.    0.288 0.051 0.001 0.145 0.351 0.001 0.\n",
      " 0.403 0.    0.    0.414 0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.291 0.311 0.327 0.344 0.366 0.379 0.392 0.399 0.412 0.416\n",
      " 0.43  0.406 0.394 0.38  0.37  0.35  0.328 0.321 0.311 0.307 0.296 0.293\n",
      " 0.283 0.279 0.273 0.266 0.26  0.248 0.244 0.235 0.225 0.218 0.214 0.211\n",
      " 0.206 0.203 0.198 0.195 0.194 0.19  0.185 0.176 0.175 0.174 0.171 0.17\n",
      " 0.168 0.167 0.163 0.162 0.16  0.16  0.157 0.157 0.153 0.152 0.149 0.144\n",
      " 0.137 0.134 0.131 0.128 0.126 0.12  0.118 0.116 0.113 0.11  0.109 0.109\n",
      " 0.107 0.106 0.105 0.105 0.103 0.101 0.1   0.1   0.098 0.097 0.095 0.094\n",
      " 0.091 0.087 0.086 0.084 0.082 0.082 0.082 0.081 0.078 0.078 0.076 0.073\n",
      " 0.071 0.069 0.068 0.068 0.067 0.066 0.064 0.062 0.061 0.061 0.06  0.058\n",
      " 0.057 0.057 0.056 0.055 0.053 0.051 0.051 0.05  0.05  0.048 0.047 0.046\n",
      " 0.045 0.043 0.041 0.04  0.037 0.035 0.033 0.032 0.032 0.031 0.029 0.028\n",
      " 0.027 0.027 0.026 0.025 0.024 0.021 0.018 0.018 0.017 0.017 0.015 0.015\n",
      " 0.014 0.013 0.013 0.013 0.012 0.011 0.01  0.009 0.009 0.008 0.008 0.008\n",
      " 0.007 0.007 0.007 0.006 0.005 0.004 0.003 0.002 0.002 0.002 0.001 0.001\n",
      " 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001\n",
      " 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.412 0.412 0.412 0.412 0.415 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412 0.412\n",
      " 0.412 0.412 0.412 0.412]\n"
     ]
    }
   ],
   "source": [
    "print(halfspace_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22ca24b-f211-48b5-baa2-c3c74586cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "halfspace_depths = halfspace(combined_x[:2000], combined_data[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7526687-c76c-4e2b-afc6-c01b0f727f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
